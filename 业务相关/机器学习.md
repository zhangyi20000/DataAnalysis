# 机器学习

重点是基于现实场景进行建模

## 实战训练

- kaggle竞赛
  - [从零开始，手把手教你玩转Kaggle比赛！](https://blog.csdn.net/weixin_48152827/article/details/145593263)

## 逻辑回归

广义线性回归模型，可以用于解决二分类问题。
例子：[身高，体重]->胖或瘦
1.1 算法原理
将一类的标签设置为0，另一类设置为1
构造极大似然函数，最小化极大似然函数（小概率原理，某一件事情发生了，就认为这件事发生的概率使最大的）

## 决策树算法

通过树形数据结构展现决策规则和分类结果，从根节点到子节点的每一个路径都代表了一种决策。
可以对数据进行分类和预测
例子：挑西瓜
1.1 两种决策树
回归决策树：对连续变量做决策树
分类决策树：对离散变量做决策树
1.2决策树的生成过程
特征选择：从众多的特种中选择一个特征作为本节点的分裂条件。
决策树生成：从上到下递归生成子节点，直到数据不可分，则决策树停止生长
决策树剪枝：决策树容易过拟合，一般需要剪枝来缩小树的规模、缓解过拟合。
1.3如何选择特征
使用信息增益作为分类的规则，信息增益越大，则选取该分裂规则。
信息增益越大，说明该特征对分类的影响程度越大。表示已知某个特征之后，类别的不确定性减少程度。
1.4 优缺点
1.简单易理解；不需要参数假设，对缺失值不敏感；可解释性
2.当类别太多是，容易出现过拟合；容易忽略数据集属性之间的相关性；对噪声敏感

## 随机森林算法

https://blog.csdn.net/sai_simon/article/details/123082619
随机森林在决策树的基础上的集成学习算法。
例子：集思广益，投票选举，少数服从多数。
3.1 有放回抽取样本K组
在总体中，有放回地抽取几组样本，作为子数据集
3.2 随机抽取特征
3.3 K个决策树做决策
3.4 结合器将K个决策进行综合
对于分类问题：少数服从多数；对于回归问题：取结果的平均值作为最后结果
3.5 优点
防止过拟合；可以处理大量输入变量，可以平衡误差。

## KNN算法---K临近算法

可以用于分类和回归，
想法：输入一个n维坐标，则在特征空间中对应1个点，输出该特征点对应的标签或数值
在表现层面：如果这几个点集离得近，那么他们就属于一类。所以说判断某个点属于哪一类：通过这个点临近点的标签进行判断属于哪个类别（投票原则）。
4.1K值的选取
先从一个较小的点开始训练，不断增加K值，绘制K值和误差曲线，找到临界点
4.2优缺点
简单明了，训练时间快，对异常值不敏感，预测效果好
内存要求高，对数据规模敏感

## 朴素贝叶斯算法

## 支持向量机算法

是一个二分类模型，可以用于分类的回归分析。对特征空间进行线性分割。分割的结果就是以一个超平面将特征空间分割成两部分，使这两部分到这个超平面的最近距离尽可能大。

## 人工神经网络

## 深度学习--机器学习的分支

卷积神经网络CNN；循环神经网络RNN，GANs；强化学习
卷积层；池化层；全连接层；